name: BJX QN Article Crawler

on:
  # Run twice daily at 9:00 AM and 9:00 PM UTC (adjust timezone as needed)
  schedule:
    - cron: '0 9 * * *'   # 9:00 AM UTC daily
    - cron: '0 21 * * *'  # 9:00 PM UTC daily
  
  # Allow manual trigger with options
  workflow_dispatch:
    inputs:
      force_full_crawl:
        description: 'Force full crawl (ignore incremental state)'
        required: false
        default: 'false'
        type: choice
        options:
          - 'false'
          - 'true'
      max_pages:
        description: 'Maximum pages to crawl'
        required: false
        default: '5'
        type: string
  
  # Run on push to main branch (for testing)
  push:
    branches: [ main ]

jobs:
  crawl:
    runs-on: ubuntu-latest
    # Alternative runners to try if region issues persist:
    # runs-on: windows-latest  # Different region/IP range
    # runs-on: macos-latest    # Different region/IP range
    
    permissions:
      contents: write  # Required to commit and push changes
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        
    - name: Install uv
      uses: astral-sh/setup-uv@v3
      with:
        version: "latest"
        
    - name: Install dependencies
      run: uv sync
      
    - name: Test network connectivity
      run: |
        echo "🔍 Testing network connectivity to target website..."
        
        # Try different DNS servers if needed
        echo "Testing with different DNS servers..."
        for dns in "8.8.8.8" "1.1.1.1" "208.67.222.222"; do
          echo "Testing with DNS: $dns"
          nslookup qn.bjx.com.cn $dns || echo "DNS $dns failed"
        done
        
        python3 -c "
        import socket
        import requests
        try:
            print('Testing DNS resolution...')
            socket.gethostbyname('qn.bjx.com.cn')
            print('✅ DNS resolution successful')
            
            print('Testing basic connectivity...')
            response = requests.get('https://qn.bjx.com.cn', timeout=15)
            print(f'✅ Basic connectivity test successful (status: {response.status_code})')
        except Exception as e:
            print(f'❌ Connectivity test failed: {e}')
            print('This might be a VPN/network issue in GitHub Actions.')
            print('Trying alternative approaches...')
            
            # Try with different User-Agent
            try:
                import requests
                headers = {
                    'User-Agent': 'Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)'
                }
                response = requests.get('https://qn.bjx.com.cn', timeout=15, headers=headers)
                print(f'✅ Alternative User-Agent successful (status: {response.status_code})')
            except Exception as e2:
                print(f'❌ Alternative approach also failed: {e2}')
                exit(1)
        "
      
    - name: Run incremental crawler
      run: uv run crawl_bjx_qn_incremental_ci.py
      env:
        MAX_PAGES: ${{ github.event.inputs.max_pages || '5' }}  # Reasonable limit for incremental crawling
        FORCE_FULL_CRAWL: ${{ github.event.inputs.force_full_crawl || 'false' }}
        OUTPUT_JSON: articles.json
        OUTPUT_CSV: articles.csv
        # Uncomment and configure if you have a proxy available:
        # HTTP_PROXY: ${{ secrets.HTTP_PROXY }}
        # HTTPS_PROXY: ${{ secrets.HTTPS_PROXY }}
        # NO_PROXY: localhost,127.0.0.1
      
    - name: Verify output files exist
      run: |
        echo "Checking for output files..."
        if [ ! -f articles.json ]; then
          echo "❌ articles.json not found!"
          ls -la
          exit 1
        fi
        if [ ! -f articles.csv ]; then
          echo "❌ articles.csv not found!"
          ls -la
          exit 1
        fi
        echo "✅ Output files verified"
        echo "📊 Articles count: $(jq length articles.json)"
        
    - name: Create timestamped directory
      run: |
        TIMESTAMP=$(date -u +"%Y-%m-%d_%H-%M-%S")
        echo "Creating timestamped backup: data/$TIMESTAMP"
        mkdir -p "data/$TIMESTAMP"
        
        # Copy files to timestamped directory
        cp articles.json "data/$TIMESTAMP/articles.json"
        cp articles.csv "data/$TIMESTAMP/articles.csv"
        cp crawl_state.json "data/$TIMESTAMP/crawl_state.json" 2>/dev/null || echo "No crawl_state.json to backup"
        
        # Keep latest files in root for easy access
        cp articles.json latest_articles.json
        cp articles.csv latest_articles.csv
        
        # Create a simple index
        echo "# Latest Crawl Results" > CRAWL_STATUS.md
        echo "" >> CRAWL_STATUS.md
        echo "**Last Updated:** $(date -u)" >> CRAWL_STATUS.md
        echo "**Articles Found:** $(jq length latest_articles.json)" >> CRAWL_STATUS.md
        echo "** Timestamp:** $TIMESTAMP" >> CRAWL_STATUS.md
        echo "" >> CRAWL_STATUS.md
        echo "## Recent Articles" >> CRAWL_STATUS.md
        echo "" >> CRAWL_STATUS.md
        jq -r '.[:5] | .[] | "- [\(.title)](\(.url)) - \(.date)"' latest_articles.json >> CRAWL_STATUS.md
        
    - name: Commit and push changes
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        git add .
        
        # Check if there are any changes
        if git diff --staged --quiet; then
          echo "No changes to commit"
        else
          git commit -m "Update articles data - $(date -u)"
          git push
        fi
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        
    - name: Prepare email content
      run: |
        # Check if crawl state exists to determine crawl type
        ARTICLE_COUNT=$(jq length latest_articles.json)
        CRAWL_TIME=$(date -u "+%Y-%m-%d %H:%M:%S UTC")
        
        # Check for new articles indication from the crawler output
        if grep -q "SUCCESS: No new articles found" <<< "$(cat /tmp/crawler_output.log 2>/dev/null || echo '')" || 
           [ -f crawl_state.json ] && [ "$ARTICLE_COUNT" -eq "$(jq -r '.total_articles_crawled // 0' crawl_state.json 2>/dev/null || echo '0')" ]; then
          # No new articles case
          cat > email_body.txt << EOF
        BJX QN Article Crawler Results - No New Articles
        
        Crawl completed successfully at: $CRAWL_TIME
        Total articles in database: $ARTICLE_COUNT
        New articles found: 0
        
        No new articles have been published since the last crawl.
        The database remains current with existing articles.
        
        ---
        Automated by GitHub Actions (Incremental Crawl)
        Repository: ${{ github.repository }}
        Run: ${{ github.run_number }}
        EOF
          echo "NO_NEW_ARTICLES=true" >> $GITHUB_ENV
        else
          # New articles found case
          cat > email_body.txt << EOF
        BJX QN Article Crawler Results
        
        Crawl completed successfully at: $CRAWL_TIME
        Total articles in database: $ARTICLE_COUNT
        
        Latest Articles:
        $(jq -r '.[:5] | .[] | "• \(.title) (\(.date))"' latest_articles.json)
        
        Please find the complete data attached as CSV file.
        
        ---
        Automated by GitHub Actions
        Repository: ${{ github.repository }}
        Run: ${{ github.run_number }}
        EOF
          echo "NO_NEW_ARTICLES=false" >> $GITHUB_ENV
        fi
        
    - name: Send email with CSV attachment
      if: success()
      uses: corysimmons/resend-email-action@v1
      with:
        api-key: ${{ secrets.RESEND_API_KEY }}
        from: BJX Crawler <notifications@resend.dev>
        to: ${{ secrets.NOTIFICATION_EMAIL }}
        subject: "BJX QN Articles - $(date -u +%Y-%m-%d) - $(jq length latest_articles.json) articles"
        text-file: email_body.txt
        attachments: latest_articles.csv
        
    - name: Send error notification
      if: failure()
      uses: corysimmons/resend-email-action@v1
      with:
        api-key: ${{ secrets.RESEND_API_KEY }}
        from: BJX Crawler <notifications@resend.dev>
        to: ${{ secrets.NOTIFICATION_EMAIL }}
        subject: "BJX QN Crawler Failed - $(date -u +%Y-%m-%d)"
        text: |
          The BJX QN article crawler failed during execution.
          
          Repository: ${{ github.repository }}
          Run: ${{ github.run_number }}
          Workflow: ${{ github.workflow }}
          
          Please check the GitHub Actions logs for details:
          ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}
          
    - name: Upload artifacts
      uses: actions/upload-artifact@v4
      with:
        name: crawler-results-${{ github.run_number }}
        path: |
          latest_articles.json
          latest_articles.csv
          CRAWL_STATUS.md
          email_body.txt
        retention-days: 30
