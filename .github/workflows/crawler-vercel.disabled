name: BJX QN Article Crawler (Vercel Backend)\n\non:\n  # Run twice daily at 9:00 AM and 9:00 PM UTC (adjust timezone as needed)\n  schedule:\n    - cron: '0 9 * * *'   # 9:00 AM UTC daily\n    - cron: '0 21 * * *'  # 9:00 PM UTC daily\n  \n  # Allow manual trigger with options\n  workflow_dispatch:\n    inputs:\n      max_pages:\n        description: 'Maximum pages to crawl'\n        required: false\n        default: '3'\n        type: string\n      force_full_crawl:\n        description: 'Force full crawl (ignore incremental state)'\n        required: false\n        default: 'false'\n        type: choice\n        options:\n          - 'false'\n          - 'true'\n\njobs:\n  crawl:\n    runs-on: ubuntu-latest\n    \n    steps:\n    - name: Checkout repository\n      uses: actions/checkout@v4\n      \n    - name: Set up Python\n      uses: actions/setup-python@v4\n      with:\n        python-version: '3.11'\n        \n    - name: Install dependencies\n      run: |\n        python -m pip install --upgrade pip\n        pip install requests jq\n        \n    - name: Call Vercel Function to crawl articles\n      id: crawl\n      run: |\n        echo \"üöÄ Calling Vercel function to crawl articles...\"\n        \n        # Build query parameters\n        MAX_PAGES=${{ github.event.inputs.max_pages || '3' }}\n        FORCE_FULL_CRAWL=${{ github.event.inputs.force_full_crawl || 'false' }}\n        QUERY_PARAMS=\"?max_pages=$MAX_PAGES&force_full_crawl=$FORCE_FULL_CRAWL\"\n        \n        # Call the Vercel function (replace with your actual Vercel URL)\n        VERCEL_URL=\"https://your-vercel-app.vercel.app/api/crawl\"\n        RESPONSE=$(curl -s -w \"\\n%{http_code}\" \"$VERCEL_URL$QUERY_PARAMS\" -H \"Content-Type: application/json\")\n        \n        HTTP_CODE=$(echo \"$RESPONSE\" | tail -n1)\n        RESPONSE_BODY=$(echo \"$RESPONSE\" | head -n -1)\n        \n        echo \"HTTP Status: $HTTP_CODE\"\n        echo \"Response: $RESPONSE_BODY\"\n        \n        # Save response for later use\n        echo \"$RESPONSE_BODY\" > vercel_response.json\n        \n        # Check if successful\n        if [ \"$HTTP_CODE\" -eq 200 ] && echo \"$RESPONSE_BODY\" | jq -e '.success' > /dev/null; then\n          echo \"‚úÖ Crawl successful\"\n          echo \"success=true\" >> $GITHUB_OUTPUT\n          echo \"articles_count=$(echo \"$RESPONSE_BODY\" | jq -r '.articlesCount')\" >> $GITHUB_OUTPUT\n        else\n          echo \"‚ùå Crawl failed\"\n          echo \"success=false\" >> $GITHUB_OUTPUT\n          ERROR_MSG=$(echo \"$RESPONSE_BODY\" | jq -r '.error // .message // \"Unknown error\"')\n          echo \"error_message=$ERROR_MSG\" >> $GITHUB_OUTPUT\n        fi\n        \n    - name: Process successful crawl results\n      if: steps.crawl.outputs.success == 'true'\n      run: |\n        echo \"üìä Processing crawl results...\"\n        \n        # Extract articles and CSV data\n        jq -r '.articles' vercel_response.json > latest_articles.json\n        echo \"$(jq -r '.csvData' vercel_response.json)\" > latest_articles.csv\n        \n        # Create timestamped directory\n        TIMESTAMP=$(date -u +\"%Y-%m-%d_%H-%M-%S\")\n        echo \"Creating timestamped backup: data/$TIMESTAMP\"\n        mkdir -p \"data/$TIMESTAMP\"\n        \n        # Copy files to timestamped directory\n        cp latest_articles.json \"data/$TIMESTAMP/articles.json\"\n        cp latest_articles.csv \"data/$TIMESTAMP/articles.csv\"\n        \n        # Create crawl status\n        cat > CRAWL_STATUS.md << EOF\n# Latest Crawl Results\n\n**Last Updated:** $(date -u)\n**Articles Found:** ${{ steps.crawl.outputs.articles_count }}\n**Timestamp:** $TIMESTAMP\n\n## Recent Articles\n\n$(jq -r '.articles[:5] | .[] | \"- [\\(.title)](\\(.url)) - \\(.date)\"' vercel_response.json)\nEOF\n        \n        echo \"‚úÖ Results processed successfully\"\n        \n    - name: Commit and push changes\n      if: steps.crawl.outputs.success == 'true'\n      run: |\n        git config --local user.email \"action@github.com\"\n        git config --local user.name \"GitHub Action\"\n        git add .\n        \n        # Check if there are any changes\n        if git diff --staged --quiet; then\n          echo \"No changes to commit\"\n        else\n          git commit -m \"Update articles data from Vercel function - $(date -u)\"\n          git push\n        fi\n      env:\n        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n        \n    - name: Prepare success email content\n      if: steps.crawl.outputs.success == 'true'\n      run: |\n        cat > email_body.txt << EOF\nBJX QN Article Crawler Results (via Vercel)\n\nCrawl completed successfully at: $(date -u \"+%Y-%m-%d %H:%M:%S UTC\")\nTotal articles in database: ${{ steps.crawl.outputs.articles_count }}\n\nLatest Articles:\n$(jq -r '.articles[:5] | .[] | \"‚Ä¢ \\(.title) (\\(.date))\"' vercel_response.json)\n\nPlease find the complete data attached as CSV file.\n\n---\nAutomated by GitHub Actions calling Vercel Function\nRepository: ${{ github.repository }}\nRun: ${{ github.run_number }}\nEOF\n        \n    - name: Prepare failure email content\n      if: steps.crawl.outputs.success == 'false'\n      run: |\n        cat > email_body.txt << EOF\nBJX QN Article Crawler FAILED (via Vercel)\n\nCrawl failed at: $(date -u \"+%Y-%m-%d %H:%M:%S UTC\")\n\nError: ${{ steps.crawl.outputs.error_message }}\n\nPlease check the GitHub Actions logs for details:\n${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}\n\n---\nAutomated by GitHub Actions calling Vercel Function\nRepository: ${{ github.repository }}\nRun: ${{ github.run_number }}\nEOF\n        \n    - name: Send email with CSV attachment (success)\n      if: steps.crawl.outputs.success == 'true'\n      uses: corysimmons/resend-email-action@v1\n      with:\n        api-key: ${{ secrets.RESEND_API_KEY }}\n        from: BJX Crawler <notifications@resend.dev>\n        to: ${{ secrets.NOTIFICATION_EMAIL }}\n        subject: \"üì∞ BJX Articles Update - $(date -u +%Y-%m-%d) - ${{ steps.crawl.outputs.articles_count }} articles\"\n        text-file: email_body.txt\n        attachments: latest_articles.csv\n        \n    - name: Send error notification (failure)\n      if: steps.crawl.outputs.success == 'false'\n      uses: corysimmons/resend-email-action@v1\n      with:\n        api-key: ${{ secrets.RESEND_API_KEY }}\n        from: BJX Crawler <notifications@resend.dev>\n        to: ${{ secrets.NOTIFICATION_EMAIL }}\n        subject: \"‚ùå BJX Crawler Failed - $(date -u +%Y-%m-%d)\"\n        text-file: email_body.txt\n        \n    - name: Upload artifacts\n      if: always()\n      uses: actions/upload-artifact@v4\n      with:\n        name: crawler-results-${{ github.run_number }}\n        path: |\n          latest_articles.json\n          latest_articles.csv\n          vercel_response.json\n          CRAWL_STATUS.md\n          email_body.txt\n        retention-days: 30