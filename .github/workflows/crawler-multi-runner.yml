name: BJX QN Article Crawler (Multi-Runner Strategy)

on:
  # Run twice daily at 9:00 AM and 9:00 PM UTC
  schedule:
    - cron: '0 9 * * *'   # 9:00 AM UTC daily
    - cron: '0 21 * * *'  # 9:00 PM UTC daily
  
  # Allow manual trigger
  workflow_dispatch:
    inputs:
      force_full_crawl:
        description: 'Force full crawl (ignore incremental state)'
        required: false
        default: 'false'
        type: choice
        options:
          - 'false'
          - 'true'
      max_pages:
        description: 'Maximum pages to crawl'
        required: false
        default: '5'
        type: string
  
  # Run on push to main branch (for testing)
  push:
    branches: [ main ]

jobs:
  # Try Windows first (better connectivity in some regions)
  crawl-windows:
    runs-on: windows-latest
    continue-on-error: true  # Don't fail the whole workflow if this fails
    
    permissions:
      contents: write
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        
    - name: Install uv
      uses: astral-sh/setup-uv@v3
      with:
        version: "latest"
        
    - name: Install dependencies
      run: uv sync
      
    - name: Test network connectivity (Windows)
      run: |
        echo "🔍 Testing network connectivity from Windows runner..."
        uv run python -c "
        import socket
        import requests
        try:
            print('Testing DNS resolution...')
            socket.gethostbyname('qn.bjx.com.cn')
            print('✅ DNS resolution successful')
            
            print('Testing basic connectivity...')
            response = requests.get('https://qn.bjx.com.cn', timeout=15)
            print(f'✅ Basic connectivity test successful (status: {response.status_code})')
        except Exception as e:
            print(f'❌ Windows connectivity test failed: {e}')
            exit(1)
        "
      
    - name: Run incremental crawler (Windows)
      run: uv run crawl_bjx_qn_incremental_ci.py
      env:
        MAX_PAGES: ${{ github.event.inputs.max_pages || '5' }}
        FORCE_FULL_CRAWL: ${{ github.event.inputs.force_full_crawl || 'false' }}
        OUTPUT_JSON: articles.json
        OUTPUT_CSV: articles.csv
      # Only run if connectivity test passed
      if: success()
      
    - name: Upload Windows results
      uses: actions/upload-artifact@v4
      if: success()
      with:
        name: crawler-results-windows
        path: |
          articles.json
          articles.csv
          crawl_state.json
        retention-days: 1

  # Try Ubuntu as fallback
  crawl-ubuntu:
    runs-on: ubuntu-latest
    needs: crawl-windows
    if: always() && needs.crawl-windows.result != 'success'  # Run if Windows didn't succeed
    continue-on-error: true
    
    permissions:
      contents: write
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        
    - name: Install uv
      uses: astral-sh/setup-uv@v3
      with:
        version: "latest"
        
    - name: Install dependencies
      run: uv sync
      
    - name: Test network connectivity (Ubuntu)
      run: |
        echo "🔍 Testing network connectivity from Ubuntu runner..."
        uv run python3 -c "
        import socket
        import requests
        try:
            print('Testing DNS resolution...')
            socket.gethostbyname('qn.bjx.com.cn')
            print('✅ DNS resolution successful')
            
            print('Testing basic connectivity...')
            response = requests.get('https://qn.bjx.com.cn', timeout=15)
            print(f'✅ Basic connectivity test successful (status: {response.status_code})')
        except Exception as e:
            print(f'❌ Ubuntu connectivity test failed: {e}')
            exit(1)
        "
      
    - name: Run incremental crawler (Ubuntu)
      run: uv run crawl_bjx_qn_incremental_ci.py
      env:
        MAX_PAGES: ${{ github.event.inputs.max_pages || '5' }}
        FORCE_FULL_CRAWL: ${{ github.event.inputs.force_full_crawl || 'false' }}
        OUTPUT_JSON: articles.json
        OUTPUT_CSV: articles.csv
      # Only run if connectivity test passed
      if: success()
      
    - name: Upload Ubuntu results
      uses: actions/upload-artifact@v4
      if: success()
      with:
        name: crawler-results-ubuntu
        path: |
          articles.json
          articles.csv
          crawl_state.json
        retention-days: 1

  # Try macOS as final fallback
  crawl-macos:
    runs-on: macos-latest
    needs: [crawl-windows, crawl-ubuntu]
    if: always() && needs.crawl-windows.result != 'success' && needs.crawl-ubuntu.result != 'success'  # Run if neither Windows nor Ubuntu succeeded
    continue-on-error: true
    
    permissions:
      contents: write
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        
    - name: Install uv
      uses: astral-sh/setup-uv@v3
      with:
        version: "latest"
        
    - name: Install dependencies
      run: uv sync
      
    - name: Test network connectivity (macOS)
      run: |
        echo "🔍 Testing network connectivity from macOS runner..."
        uv run python3 -c "
        import socket
        import requests
        try:
            print('Testing DNS resolution...')
            socket.gethostbyname('qn.bjx.com.cn')
            print('✅ DNS resolution successful')
            
            print('Testing basic connectivity...')
            response = requests.get('https://qn.bjx.com.cn', timeout=15)
            print(f'✅ Basic connectivity test successful (status: {response.status_code})')
        except Exception as e:
            print(f'❌ macOS connectivity test failed: {e}')
            exit(1)
        "
      
    - name: Run incremental crawler (macOS)
      run: uv run crawl_bjx_qn_incremental_ci.py
      env:
        MAX_PAGES: ${{ github.event.inputs.max_pages || '5' }}
        FORCE_FULL_CRAWL: ${{ github.event.inputs.force_full_crawl || 'false' }}
        OUTPUT_JSON: articles.json
        OUTPUT_CSV: articles.csv
      # Only run if connectivity test passed
      if: success()
      
    - name: Upload macOS results
      uses: actions/upload-artifact@v4
      if: success()
      with:
        name: crawler-results-macos
        path: |
          articles.json
          articles.csv
          crawl_state.json
        retention-days: 1

  # Process results from whichever runner succeeded
  process-results:
    runs-on: ubuntu-latest
    needs: [crawl-windows, crawl-ubuntu, crawl-macos]
    if: always()  # Run even if some jobs failed
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Download Windows results
      uses: actions/download-artifact@v4
      if: needs.crawl-windows.result == 'success'
      with:
        name: crawler-results-windows
        path: ./windows-results
        
    - name: Download Ubuntu results
      uses: actions/download-artifact@v4
      if: needs.crawl-ubuntu.result == 'success'
      with:
        name: crawler-results-ubuntu
        path: ./ubuntu-results
        
    - name: Download macOS results
      uses: actions/download-artifact@v4
      if: needs.crawl-macos.result == 'success'
      with:
        name: crawler-results-macos
        path: ./macos-results
        
    - name: Select successful results
      run: |
        if [ -d "./windows-results" ]; then
          echo "Using Windows results"
          cp ./windows-results/* ./
        elif [ -d "./ubuntu-results" ]; then
          echo "Using Ubuntu results"
          cp ./ubuntu-results/* ./
        elif [ -d "./macos-results" ]; then
          echo "Using macOS results"
          cp ./macos-results/* ./
        else
          echo "❌ All runners failed - creating empty files"
          echo "[]" > articles.json
          echo "title,date,url" > articles.csv
        fi
        
    - name: Verify output files exist
      run: |
        echo "Checking for output files..."
        if [ ! -f articles.json ]; then
          echo "❌ articles.json not found!"
          ls -la
          exit 1
        fi
        if [ ! -f articles.csv ]; then
          echo "❌ articles.csv not found!"
          ls -la
          exit 1
        fi
        echo "✅ Output files verified"
        echo "📊 Articles count: $(jq length articles.json)"
        
    - name: Create timestamped directory
      run: |
        TIMESTAMP=$(date -u +"%Y-%m-%d_%H-%M-%S")
        echo "Creating timestamped backup: data/$TIMESTAMP"
        mkdir -p "data/$TIMESTAMP"
        
        # Copy files to timestamped directory
        cp articles.json "data/$TIMESTAMP/articles.json"
        cp articles.csv "data/$TIMESTAMP/articles.csv"
        cp crawl_state.json "data/$TIMESTAMP/crawl_state.json" 2>/dev/null || echo "No crawl_state.json to backup"
        
        # Keep latest files in root for easy access
        cp articles.json latest_articles.json
        cp articles.csv latest_articles.csv
        
        # Create a simple index
        echo "# Latest Crawl Results" > CRAWL_STATUS.md
        echo "" >> CRAWL_STATUS.md
        echo "**Last Updated:** $(date -u)" >> CRAWL_STATUS.md
        echo "**Articles Found:** $(jq length latest_articles.json)" >> CRAWL_STATUS.md
        echo "**Timestamp:** $TIMESTAMP" >> CRAWL_STATUS.md
        echo "" >> CRAWL_STATUS.md
        echo "## Recent Articles" >> CRAWL_STATUS.md
        echo "" >> CRAWL_STATUS.md
        jq -r '.[:5] | .[] | "- [\(.title)](\(.url)) - \(.date)"' latest_articles.json >> CRAWL_STATUS.md
        
    - name: Commit and push changes
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        git add .
        
        # Check if there are any changes
        if git diff --staged --quiet; then
          echo "No changes to commit"
        else
          git commit -m "Update articles data - $(date -u)"
          git push
        fi
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        
    - name: Send success notification
      if: success()
      uses: corysimmons/resend-email-action@v1
      with:
        api-key: ${{ secrets.RESEND_API_KEY }}
        from: BJX Crawler <notifications@resend.dev>
        to: ${{ secrets.NOTIFICATION_EMAIL }}
        subject: "✅ BJX QN Articles - $(date -u +%Y-%m-%d) - $(jq length latest_articles.json) articles"
        text: |
          BJX QN Article Crawler Results
          
          Crawl completed successfully at: $(date -u "+%Y-%m-%d %H:%M:%S UTC")
          Total articles in database: $(jq length latest_articles.json)
          
          Repository: ${{ github.repository }}
          Run: ${{ github.run_number }}
          
          ---
          Automated by GitHub Actions (Multi-Runner Strategy)
        attachments: latest_articles.csv
        
    - name: Send failure notification
      if: failure()
      uses: corysimmons/resend-email-action@v1
      with:
        api-key: ${{ secrets.RESEND_API_KEY }}
        from: BJX Crawler <notifications@resend.dev>
        to: ${{ secrets.NOTIFICATION_EMAIL }}
        subject: "❌ BJX QN Crawler Failed - $(date -u +%Y-%m-%d)"
        text: |
          The BJX QN article crawler failed on all runners.
          
          This indicates a potential network connectivity issue that affects all GitHub Actions regions.
          
          Repository: ${{ github.repository }}
          Run: ${{ github.run_number }}
          
          Please check the GitHub Actions logs for details:
          ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}
